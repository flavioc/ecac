\documentclass[a4paper]{llncs}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[utf8]{inputenc}
\usepackage{subfigure}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{listings}
\usepackage[hyphens]{url}
\urldef{\mails}\path|{ei05011,ei05028}@fe.up.pt|
\bibliographystyle{splncs}
\begin{document}
\title{A comparison of classification algorithms applied to the Adult Database}
\author{Flávio Cruz \and João Azevedo}
\institute{Faculdade de Engenharia da Universidade do Porto\\
    Rua Dr. Roberto Frias, s/n 4200-465 Porto PORTUGAL\\
    \mails}
\maketitle

\begin{abstract}
Highly frequent in experimental tests related to data mining, the Adult Database
has some interesting properties which make it quite useful to test the 
efficiency of classification algorithms. This report aims to describe the Adult
Database from a statistical perspective and taking into account its support for
data mining tasks.
\end{abstract}

\section{Introduction}

\subsection{Problem Identification}

Deducing from the previous section, this is a classification problem. Given an adult
with the mentioned attributes we want to classify him into one of the following groups:
one gains more than 50000 dollars per year, the other group does not.

One important objective in classifying things is to understand why the given object
is put under one class and not in another \cite{2}\cite{3}, that is, we want to understand what social characteristics
allows one person to gain more than someone else, if the sex is important, the education level,
the race and what factors are less important.

Given a specific set of persons from a certain society and from a certain time we could then
learn what contributes to a person's social standing and, as we all know, income is an important
metric in that department. We could, for example, learn if the race affects the persons success and if
that society discriminates against certain races.

Understanding what affects the income can give a lot of information concerning society's behaviors.

\section{Algorithms} \label{sec:algs}

The algorithms we used to build classifiers can be grouped into four main
groups: decision trees, rule induction \cite{rule_induction},
inductive logic programming and bayesian classifiers \cite{bayes}.

\subsection{Decision trees}

For decision trees, the algorithms used are ID3 \cite{id3}, C4.5 \cite{c45}, CART \cite{cart},
Random Forest \cite{random_forest}, and alternating decision trees (adtree) \cite{adtree}.

Decision tree learning is a very commonly used method in classification. The main goal here is to
create a tree model that can predict an object's class. Each interior node corresponds
to one test for the object's attributes and each edge links to new interiors nodes based on the corresponding
results. Each leaf in the tree represents the object class for the tests made
across the respective path, from root to leaf.

The ID3 algorithm (Iterative Dichotomiser 3) was one of the first decision trees algorithms.
It was designed by Ross Quinlan.

The C4.5 algorithm is the improved version of the ID3 algorithm, also designed by Quinlan.
Contrary to ID3, this algorithm can handle both continuous and discrete attributes,
it can use data with missing values and can also prune its decision trees.

The CART algorithm builds classification and regression trees for predicting continuous dependent variables (regression)
and categorical predictor variables (classification). The classic CART algorithm was popularized by Breiman et al. \cite{cart}

The alternating decision tree is another type of decision tree algorithms. This tree
consists of decision nodes, which specify a predicate condition, and prediction nodes, containing a single number.
Each object is classified by following all paths for which all decision nodes are true and summing any
prediction nodes along the way. Contrary to binary classification like ID3 ou C4.5, in adtrees one
object can follow multiple paths. 

Finally, the Random Forest algorithm is an ensemble classifier which consists of many decision trees.
Each object is classified multiple times by all decision trees and the mode class is used as the final class.
Like the CART algorithm, Random Forest was also developed by Leo Breiman.

\subsection{Rule induction}

To experiment with rule induction algorithms we chose the CN2 \cite{cn2} algorithm.

Rule induction works by extracting formal rules from a set of observations.
Usually rules are expressed in the form \textit{if condition1 and condition2 ... then (decision, value)}.

CN2 is an algorithm for inducing 
propositional classification rules. CN2 consists of two main procedures: the 
search procedure that performs beam search in order to find a single rule and 
the control procedure that repeatedly executes the search. 
The search procedure performs beam search using classification accuracy 
of the rule as a heuristic function.
Two different control procedures are used in CN2: one for inducing an ordered 
list of rules and the other for the unordered case.

\subsection{Inductive Logic Programming}

Hypothesis construction is the core of any ILP system and there exists many different techniques for constructing them, including: Inverse Resolution, Relative Least General Generalisations, Inverse Implication, and Inverse Entailment.

We will be using the Aleph system \cite{aleph} to experiment with inductive logic programming. This system
uses the Inverse Entailment algorithm and has also incorporated features from many of its predecessors based on other techniques.

\subsection{Bayesian classifiers}

For bayesian classifiers we tested different kinds of search algorithms to build
the network structure.

The construction of the network structured is also known as
\textit{local score metrics} and various algorithms can be employed for this optimization problem.

The experiments conducted will use genetic based search, hill climbing \cite{bayes3}, simulated annealing,
and a modified hill climbing algorithm, named K2 \cite{bayes2}.

For the second step in building a bayesian network classifier, we used a
simple estimator to estimate the conditional probability distributions.
This simple estimator produces
direct estimates of the conditional probabilities. \cite{bayes_weka}

\section{Data Format}

The ``Adult Database'' consists of 48842 entries of individual information of 
people gathered in a 1994 census on the United States of America. From the
global collection of data, a representative sample was collected from people
with ages between 16 and 100 and a final weight above 1.

The data set is divided in two groups: the train set with 32561 records and the test set with 16281 entries.

Each entry is composed by a set of 14 attributes and two classes, identifying
persons who gain more or less than 50K dollars per year. The attributes and types are:

\begin{itemize}
  \item{\textbf{Age}: continuous.}
  \item{\textbf{Workclass}: categorical: [[Private], [Self-emp-not-inc], 
        [Self-emp-inc], [Federal-gov], [Local-gov], [State-gov], [Without-pay], 
        [Never-worked]]}
  \item{\textbf{FNLWGT}: continuous. Represents the final weight. People with 
        the same demographical characteristics should have the same weight.}
  \item{\textbf{Education}: ordinal: [1. [Preschool], 2. [1st-4th], 3. 
        [5th-6th], 4. [7th-8th], 5. [9th < 10th], 6. [11th < 12th], 7. 
        [HS-grad], 8. [Prof-school], 9. [Assoc-acdm], 10. [Assoc-voc], 11.
        [Some-college], 12. [Bachelors], 13. [Masters], 14. [Doctorate]]}
  \item{\textbf{Education-num}: continuous. Is a continuous representation of 
        the \textbf{Education} attribute.}
  \item{\textbf{Marital-status}: categorical: [[Married-civ-spouse], [Divorced],
        [Never-married], [Separated], [Widowed], [Married-spouse-absent],
        [Married-AF-spouse]]}
  \item{\textbf{Occupation}: categorical: [[Tech-support], [Craft-repair],
        [Other-service], [Sales], [Exec-managerial], [Prof-specialty], 
        [Handlers-cleaners], [Machine-op-inspct], [Adm-clerical], 
        [Farming-fishing], [Transport-moving], [Priv-house-serv], 
        [Protective-serv], [Armed-Forces]]}
  \item{\textbf{Relationship}: categorical: [[Wife], [Own-child], [Husband], 
        [Not-in-family], [Other-relative], [Unmarried]]}
  \item{\textbf{Race}: categorial: [[White], [Asian-Pac-Islander], 
        [Amer-Indian-Eskimo], [Other], [Black]]}
  \item{\textbf{Sex}: categorical: [[Male], [Female]]}
  \item{\textbf{Capital-gain}: continuous.}
  \item{\textbf{Capital-loss}: continuous.}
  \item{\textbf{Hours-per-week}: continuous.}
  \item{\textbf{Native-country}: categorical.}
\end{itemize}

There are 3620 records with missing values. The attributes that contain missing values are: \textbf{Workclass}, \textbf{Occupation} and \textbf{Native-country}.

This data set is available in the following URL: \url{ftp://ftp.ics.uci.edu/pub/machine-learning-databases/adult}.

\section{Experimental Context}

In this section we will describe the experimental context for each
algorithm described in the section \ref{sec:algs}, namely, programs we used,
preprocessing tasks conducted, the parameterization that is available for each
algorithm and so forth.

The data set available in \url{ftp://ftp.ics.uci.edu/pub/machine-learning-databases/adult}
is composed of two CSV files, one for the training set and the other for the test set.
Each file was imported into a MySQL database and grouped into two tables: adult and adult\_test.
These tables follow the same structure (Table \ref{tbl:mysql_table}).

\begin{table}[ht]
  \begin{tabular}{ | l | p{14cm} |}
    \hline
    \textbf{Field} & \textbf{Type} \\ \hline
    id & SERIAL \\ \hline
    age & INT \\ \hline
    workclass & enum('private', 'self\_emp\_not\_inc', 'self\_emp\_inc','federal\_gov', 'local\_gov', 'state\_gov', 'without\_pay', 'never\_worked', 'unknown') \\ \hline
    fnlwgt & INT \\ \hline
    education & enum('bachelors', 'some\_college', '11th', 'hs\_grad', 'prof\_school', 'assoc\_acdm', 'assoc\_voc', '9th', '7th\_8th', '12th', 'masters', '1st\_4th', '10th', 'doctorate', '5th\_6th', 'preschool') \\ \hline
    education\_num & INT \\ \hline
    marital\_status & enum('married\_civ\_spouse', 'divorced', 'never\_married', 'separated', 'widowed', 'married\_spouse\_absent', 'married\_af\_spouse') \\ \hline
    occupation & enum('tech\_support', 'craft\_repair', 'other\_service', 'sales', 'exec\_managerial', 'prof\_specialty', 'handlers\_cleaners', 'machine\_op\_inspct', 'adm\_clerical', 'farming\_fishing', 'transport\_moving', 'priv\_house\_serv', 'protective\_serv', 'armed\_forces', 'unknown') \\ \hline
    relationship & enum('wife', 'own\_child', 'husband', 'not\_in\_family', 'other\_relative', 'unmarried') \\ \hline
    race & enum('white', 'asian\_pac\_islander', 'amer\_indian\_eskimo', 'other', 'black') \\ \hline
    sex & enum('male', 'female') \\ \hline
    capital\_gain & INT \\ \hline
    capital\_loss & INT \\ \hline
    hours\_per\_week & INT \\ \hline
    native\_country & enum('united\_states', 'cambodia', 'england', 'puerto\_rico', 'canada', 'germany', 'outlying\_us', 'india', 'japan', 'greece', 'south', 'china', 'cuba', 'iran', 'honduras', 'philippines', 'italy', 'poland', 'jamaica', 'vietnam', 'mexico', 'portugal', 'ireland', 'france', 'dominican\_republic', 'laos', 'ecuador', 'taiwan', 'haiti', 'columbia', 'hungary', 'guatemala', 'nicaragua', 'scotland', 'thailand', 'yugoslavia', 'el\_salvador', 'trinadad\_tobago', 'peru', 'hong', 'holand\_netherlands', 'unknown') \\ \hline
    plus\_50 & BOOLEAN \\ \hline
  \end{tabular}
  \caption{MySQL table structure.}
  \label{tbl:mysql_table}
\end{table}

For the majority of the proposed algorithms we tried to make use of the database, instead
of using the CSV files, as it allows a more flexible approach to data querying using SQL.

\subsection{Weka}
\label{sec:weka}

Weka is a collection of machine learning algorithms for data mining tasks. \cite{weka}
It can be applied directly to a dataset using command line tools or through Java code, using the Weka Java APIs.
Weka supports tools for data pre-processing, classification, regression, clustering, association rules,
and visualization.

This tool was used to apply the following algorithms: C4.5, ADTree, Random Forest, CART and Bayesian Networks.
Instead of using the command line tools, we used the Java API to build classifiers.

To use the data set we fetch it directly from the database and then apply various data pre-processing
techniques that are available within Weka:

\begin{itemize}
  \item The \textbf{plus\_50} was converted from a numerical format to categories: $<$50K and $>$50K.
  \item Every field marked as \textbf{unknown} was marked as \textbf{missing}.
  \item The \textbf{age} was discretized into various categories: \textbf{young} ($<$25),
    \textbf{middle\_aged} (25-45), \textbf{senior} (45-65), and \textbf{old} ($>$65).
  \item The attribute \textbf{hours\_per\_week} was also discretized: \textbf{part\_time} ($<$25),
    \textbf{full\_time} (25-40), \textbf{over\_time} (40-60), and \textbf{workaholic} ($>$60).
  \item Each capital attribute (\textbf{capital\_gain} and \textbf{capital\_loss}) was also discretized:
    \textbf{none} (0), \textbf{very\_low} (0-100), \textbf{low} (100-1000), \textbf{medium} (1000-5000),
    \textbf{high} (5000-10000), and \textbf{very\_high} ($>$10000).
\end{itemize}

Only the following values are feed into the algorithms: age, workclass, education,
marital\_status, occupation, relationship, race, sex, capital\_gain, capital\_loss,
hours\_per\_week, native\_country, and plus\_50. The attributes education\_num
and fnlwgt are derived from other attributes and so we keep them out.

For missing values we used the \textit{ReplaceMissingValues} filter available in Weka that
replaces unknown values with the modes and means from the data set. 

Each algorithm has various configuration parameters that will be explored
and analyzed in section \ref{sec:experimental_eval}.
For example, the Bayesian Network
algorithm allows the parameterization of the search algorithm that optimizes the network structure.

After the data as been pre-processed,
we will build the classifier using the train data set.
Once the classifier is trained, it will be used to classify both the train and test data.

Performance metrics for both classification results will be registered: error rate and time spent
building the classifier and classifying all the examples.

\subsection{RapidMiner}

RapidMiner is an environment for machine learning and data mining experiments 
\cite{4}. Experiments are done by composing a process made up of a large amount
of nestable operators. This kind of approach makes it very suitable to create
and modify different kinds of experiments in a quick fashion.

RapidMiner provides both a GUI to design the process pipeline and a command-line
tool to load XML-based configuration files. When using the GUI, users can easily
switch between the pipeline visualization and the corresponding XML file,
allowing for a better understanding of the configuration file internals. A large
number of different outputs can be configured on a RapidMiner's process 
pipeline, providing a broad range of visualizations for the final results.
RapidMiner also integrates learning schemes and attribute evaluators from the 
Weka \cite{weka} learning environment (see section \ref{sec:weka} for more 
details) and provides a programming API for use in different environments.

This tool was used to apply the ID3 \cite{id3}\cite{1} algorithm, mainly because
of its support for numerical attributes (not seen in Weka), wich enabled a
better comparison of approaches.

\subsection{CN2}

CN2 \cite{cn2} is a learning algorithm for rule induction \cite{rule_induction}
that isn't available in Weka nor in RapidMiner. CN2 inductively learns a set of
propositional if...then.. rules from a set of training examples, performing a
general-to-specific beam search through the rule space for the "best" rule, 
removing training examples covered by it, and repeating it until no more "good"
rules can be found. The original version of the algorithm used entropy and
significance test to decide the best rule. Later, it was improved to use the
Laplace estimate \cite{cn2_laplace}, which is implemented in Peter Clark's 
software used for our testing purposes.

Peter Clark's CN2 software is built in C and provides a command-line environment
for conducting experiments. The input format is limited to two files, one
defining the attributes and other defining the training examples. The software 
also provides an environment for performance evaluation of the produced set of 
rules, by accepting a test file with examples.

Only the following values are feed into the algorithms: age, workclass, 
education, marital\_status, occupation, relationship, race, sex, capital\_gain,
capital\_loss, hours\_per\_week, native\_country, and plus\_50. The attributes
education\_num and fnlwgt are derived from other attributes and so are kept out.
In terms of data pre-processing for missing values, continuous values were 
replaced by the average value, while discrete values were replaced by the mode.

In terms of performance metrics, we took into account the error rate and the 
time spent building the rule set.

\subsection{Aleph}

Aleph \cite{aleph} is currently one of the most widely used ILP \cite{ilp} 
systems. Developed by Ashwin Srinivasan is, as of 2007, in its 5th version. The
basic Aleph algorithm (used on our experiments) consists of 4 steps:

\begin{enumerate}
    \item{Select example}
    \item{Build most-specific clause}
    \item{Search}
    \item{Remove redundant}
\end{enumerate}

Aleph uses inverse entailment to build the most-specific clause and its basic
algorithm uses a best-first search at the search step. Advanced use of Aleph 
allows tweaks in every step of the algorithm, namely:

\begin{itemize}
    \item{\textbf{Select example}: The size of the sample can be configured.}
    \item{\textbf{Build most-specific clause}: The approach on building the 
    bottom-clause can be configured. The evaluation mechanism can also be 
    tweaked.}
    \item{\textbf{Search}: A huge range of search strategies, evaluation 
    functions and refinement operators can be applied.}
    \item{\textbf{Remove redundant}: A strategy of retaining covered examples
    can be applied, for a better estimation of clause scores.}
\end{itemize}

Aleph is built entirely in prolog and is best suited to run on the YAP Prolog 
engine. 

In order to use ILP to build a classifier for our data set, some data
pre-processing needs to be run, namely on converting the examples to logic 
predicates. Despite the fact that Aleph is able to deal with numerical values,
discretization has been run on the variables ``age'', ``capital\_gain'', 
``capital\_loss'' and ``hours\_per\_week''. The value distribution was the same
as described in section \ref{sec:weka}. The main classification clause is given
by \verb$more_than_50K(+person).$ Those persons winning more than 50K per year
are the positive examples, the rest are the negative examples. Each of the
attributes was converted to unary prolog predicates (for example 
\verb$young(+person).$, \verb$work_private(+person).$, \verb$male(+person).$,
\verb$is_native_of_portugal(+person).$). Missing values were simply ommited from
the background knowledge.

In terms of performance metrics, we took into account the error rate and the 
time spent building the hypothesis set. The expressiveness of the results is
also analyzed in section \ref{sec:experimental_eval}.

\section{Experimental Evaluation} \label{sec:experimental_eval}

This section shows performance results for the various algorithms.

\subsection{C4.5}

C4.5 in Weka provides a number of parameters to tune the algorithm.
One of these parameters allows the creation of unpruned trees $<$\textbf{-U}$>$.
For pruned trees we can use the confidence threshold parameter that dictates how
much pruning the algorithm will do $<$\textbf{-C float}$>$.

We executed the algorithm using different combinations of the parameters described above.
Table \ref{tbl:results_c45} displays the obtained results. From the table
we note that once the confidence threshold reaches a certain level ($\approx$0.55) the results
are very similar for the unpruned classifier and the over-fitting effect starts to get worse,
where the train data gets very good results ($\approx$11.19 error rate) and the test set gets a
large error rate ($\approx$24).

The confidence threshold for best test error rates appears to be in the interval 0.05-0.15.   

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} & \textbf{Nodes} & \textbf{Leaves} \\ \hline
    -U & 11.18 & 23.74 & 0m24.378s & 6919 & 6198 \\ \hline
    -C 0.05 & 15.67 & 22.86 & 0m24.414s & 46 & 38 \\ \hline
    -C 0.1 & 15.22 & 21.69 & 0m25.417s & 106 & 89 \\ \hline
    -C 0.15 & 14.85 & 22.25 & 0m25.086s & 186 & 158 \\ \hline
    -C 0.2 & 14.52 & 22.05 & 0m24.436s & 298 & 254 \\ \hline
    -C 0.25 & 14.40 & 22.08 & 0m24.628s & 376 & 319 \\ \hline
    -C 0.3 & 14.17 & 23.26 & 0m24.765s & 494 & 423 \\ \hline
    -C 0.35 & 13.84 & 22.42 & 0m25.154s & 692 & 593 \\ \hline
    -C 0.4 & 13.68 & 22.49 & 0m24.325s & 835 & 721 \\ \hline
    -C 0.45 & 13.26 & 22.43 & 0m25.086s & 1143 & 992 \\ \hline
    -C 0.5 & 12.95 & 22.51 & 0m24.874s & 1483 & 1292 \\ \hline
    -C 0.55 & 11.19 & 24.94 & 0m25.899s & 6727 & 6036 \\ \hline
    -C 0.6 & 11.19 & 23.94 & 0m26.882s & 6727 & 6036 \\ \hline
    -C 0.7 & 11.19 & 23.94 & 0m26.638s & 6727 & 6036 \\ \hline
    -C 0.8 & 11.19 & 23.94 & 0m26.818s & 6727 & 6036 \\ \hline
    -C 0.9 & 11.19 & 23.94 & 0m26.634s & 6727 & 6036 \\ \hline
  \end{tabular}
  \caption{Results for C4.5.}
  \label{tbl:results_c45}
  \end{center}
\end{table}

A decision tree built by this algorithm with the confidence threshold set to 0.05 is shown in Listing \ref{decision_tree_c45}.

\section{Related Work}

\section{Conclusions}

\begin{thebibliography}{1}
\bibitem{1}
Mitchel, T.; Machine Learning; McGraw-Hill, 1997
\bibitem{2}
Han, J., Kamber, M., Pei, J.; Data Mining: Concepts and Techniques, Second Edition; Morgan Kaufmann, 2005
\bibitem{3}
H. Witten, I., Frank, E.; Data Mining: Practical Machine Learning Tools and Techniques, Second Edition; Morgan Kaufmann, 2005
\bibitem{4}
RapidMiner; \url{http://www.rapidminer.com/}, November 2009
\bibitem{c45}
Ross Quinlan, J.; C4.5: Programs for Machine Learning; Morgan Kaufmann, 1992
\bibitem{6}
The R Project for Statistical Computing; http://www.r-project.org/, November 2009
\bibitem{7}
Hahsler, M., Gruen, B.; Hornik, K.; arules -- A Computational Environment for Mining Association Rules and Frequent Item Sets; Journal of Statistical Software 14/15, 2005

\bibitem{id3}
Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.

\bibitem{cart}
Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (1984). Classification and Regression Trees. Wadsworth International Group, Belmont, California.

\bibitem{random_forest}
Liaw, Andy and Wiener, Matthew. Classification and Regression by randomForest. R News (2002) Vol. 2/3 p. 18

\bibitem{adtree}
Freund, Y., Mason, L.: The alternating decision tree learning algorithm. In: Proceeding of the Sixteenth International Conference on Machine Learning, Bled, Slovenia, 124-133, 1999

\bibitem{bayes}
N. Friedman, D. Geiger, M. Goldszmidt. Bayesian Network Classifiers. Machine Learning, 29: 131–163, 1997

\bibitem{rule_induction}
Quinlan, J. R. (1987). "Generating production rules from decision trees". in McDermott, John. Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87). Milan, Italy. pp. 304–307

\bibitem{aleph}
Ashwin Srinivasan. The Aleph Manual. 2007

\bibitem{bayes2}
G. Cooper, E. Herskovits. A Bayesian method for the induction of probabilistic networks from 
data. Machine Learning, 9: 309–347, 1992.

\bibitem{bayes3}
W.L. Buntine. A guide to the literature on learning probabilistic networks from data. IEEE 
Transactions on Knowledge and Data Engineering, 8:195–210, 1996.

\bibitem{bayes_weka}
Remco R. Bouckaert. Bayesian Network Classifiers in Weka. 2004

\bibitem{weka}
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten (2009); The WEKA Data Mining Software: An Update; SIGKDD Explorations, Volume 11, Issue 1.

\bibitem{cn2}
Peter Clark and Tim Niblett. The CN2 induction algorithm. 1988.

\bibitem{cn2_laplace}
Peter Clark and Robin Boswell. Rule induction with CN2: Some recent improvements. In Y. Kodratoff, editor, Machine Learning - EWSL-91, pages 151-163, Berlin, 1991. Springer-Verlag.

\bibitem{ilp}
Nada Lavrac and Saso Dzeroski. Inductive Logic Programming: Techniques and Applications. Ellis Horwood, New York, 1994.

\end{thebibliography}

\clearpage

\appendix

\section{Tables}

\clearpage
\section{Decision trees}

\begin{lstlisting}[language=c,frame=single,breaklines=true,basicstyle=\footnotesize\ttfamily,caption={Decision tree built by C4.5 with -C 0.05},label=decision_tree_c45]
capital_gain = none
|   marital_status = never_married: <50K (10228.0/340.0)
|   marital_status = married_civ_spouse
|   |   education = bachelors
|   |   |   capital_loss = none
|   |   |   |   age = young: <50K (33.0/4.0)
|   |   |   |   age = middle_aged: >50K (1287.0/515.0)
|   |   |   |   age = senior: >50K (707.0/243.0)
|   |   |   |   age = old: <50K (77.0/25.0)
|   |   |   capital_loss = low: >50K (0.0)
|   |   |   capital_loss = medium: >50K (222.0/26.0)
|   |   education = hs_grad
|   |   |   capital_loss = none: <50K (4167.0/1130.0)
|   |   |   capital_loss = low: <50K (1.0)
|   |   |   capital_loss = medium: >50K (235.0/105.0)
|   |   education = 11th: <50K (328.0/29.0)
|   |   education = masters: >50K (823.0/214.0)
|   |   education = 9th: <50K (212.0/18.0)
|   |   education = some_college: <50K (2508.0/1000.0)
|   |   education = assoc_acdm: <50K (412.0/186.0)
|   |   education = assoc_voc: <50K (596.0/251.0)
|   |   education = 7th_8th: <50K (325.0/29.0)
|   |   education = doctorate: >50K (230.0/42.0)
|   |   education = prof_school: >50K (295.0/62.0)
|   |   education = 5th_6th: <50K (161.0/11.0)
|   |   education = 10th
|   |   |   capital_loss = none: <50K (314.0/41.0)
|   |   |   capital_loss = low: <50K (0.0)
|   |   |   capital_loss = medium: >50K (9.0/2.0)
|   |   education = 1st_4th: <50K (78.0/5.0)
|   |   education = preschool: <50K (18.0)
|   |   education = 12th: <50K (119.0/21.0)
|   marital_status = divorced: <50K (4155.0/333.0)
|   marital_status = married_spouse_absent: <50K (397.0/25.0)
|   marital_status = separated: <50K (973.0/46.0)
|   marital_status = married_af_spouse
|   |   hours_per_week = part_time: <50K (2.0)
|   |   hours_per_week = full_time: <50K (3.0)
|   |   hours_per_week = over_time: >50K (13.0/4.0)
|   |   hours_per_week = workaholic: <50K (3.0)
|   marital_status = widowed: <50K (918.0/62.0)
capital_gain = low: <50K (55.0)
capital_gain = medium: <50K (1009.0/181.0)
capital_gain = high: >50K (878.0/138.0)
capital_gain = very_high: >50K (770.0/14.0)
\end{lstlisting}

\end{document}
