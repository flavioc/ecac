\documentclass[a4paper]{llncs}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[utf8]{inputenc}
\usepackage{subfigure}
\usepackage{a4wide}
\usepackage{listings}
\usepackage[hyphens]{url}
\urldef{\mails}\path|{ei05011,ei05028}@fe.up.pt|
\bibliographystyle{splncs}
\begin{document}
\title{A comparison of classification algorithms applied to the Adult Database}
\author{Flávio Cruz \and João Azevedo}
\institute{Faculdade de Engenharia da Universidade do Porto\\
    Rua Dr. Roberto Frias, s/n 4200-465 Porto PORTUGAL\\
    \mails}
\maketitle

\begin{abstract}
Highly frequent in experimental tests related to data mining, the Adult Database
has some interesting properties which make it quite useful to test the 
efficiency of classification algorithms. This report aims to describe the Adult
Database from a statistical perspective and taking into account its support for
data mining tasks.
\end{abstract}

\section{Introduction}

\subsection{Problem Identification}

Deducing from the previous section, this is a classification problem. Given an adult
with the mentioned attributes we want to classify him into one of the following groups:
one gains more than 50000 dollars per year, the other group does not.

One important objective in classifying things is to understand why the given object
is put under one class and not in another \cite{2}\cite{3}, that is, we want to understand what social characteristics
allows one person to gain more than someone else, if the sex is important, the education level,
the race and what factors are less important.

Given a specific set of persons from a certain society and from a certain time we could then
learn what contributes to a person's social standing and, as we all know, income is an important
metric in that department. We could, for example, learn if the race affects the persons success and if
that society discriminates against certain races.

Understanding what affects the income can give a lot of information concerning society's behaviors.

\section{Algorithms} \label{sec:algs}

The algorithms we used to build classifiers can be grouped into four main
groups: decision trees, rule induction \cite{rule_induction},
inductive logic programming and bayesian classifiers \cite{bayes}.

\subsection{Decision trees}

For decision trees, the algorithms used are ID3 \cite{id3}, C4.5 \cite{c45}, CART \cite{cart},
Random Forest \cite{random_forest}, and alternating decision trees (adtree) \cite{adtree}.

Decision tree learning is a very commonly used method in classification. The main goal here is to
create a tree model that can predict an object's class. Each interior node corresponds
to one test for the object's attributes and each edge links to new interiors nodes based on the corresponding
results. Each leaf in the tree represents the object class for the tests made
across the respective path, from root to leaf.

The ID3 algorithm (Iterative Dichotomiser 3) was one of the first decision trees algorithms.
It was designed by Ross Quinlan.

The C4.5 algorithm is the improved version of the ID3 algorithm, also designed by Quinlan.
Contrary to ID3, this algorithm can handle both continuous and discrete attributes,
it can use data with missing values and can also prune its decision trees.

The CART algorithm builds classification and regression trees for predicting continuous dependent variables (regression)
and categorical predictor variables (classification). The classic CART algorithm was popularized by Breiman et al. \cite{cart}

The alternating decision tree is another type of decision tree algorithms. This tree
consists of decision nodes, which specify a predicate condition, and prediction nodes, containing a single number.
Each object is classified by following all paths for which all decision nodes are true and summing any
prediction nodes along the way. Contrary to binary classification like ID3 ou C4.5, in adtrees one
object can follow multiple paths. 

Finally, the Random Forest algorithm is an ensemble classifier which consists of many decision trees.
Each object is classified multiple times by all decision trees and the mode class is used as the final class.
Like the CART algorithm, Random Forest was also developed by Leo Breiman.

\subsection{Rule induction}

To experiment with rule induction algorithms we chose the CN2 \cite{cn2} algorithm.

Rule induction works by extracting formal rules from a set of observations.
Usually rules are expressed in the form \textit{if condition1 and condition2 ... then (decision, value)}.

CN2 is an algorithm for inducing 
propositional classification rules. CN2 consists of two main procedures: the 
search procedure that performs beam search in order to find a single rule and 
the control procedure that repeatedly executes the search. 
The search procedure performs beam search using classification accuracy 
of the rule as a heuristic function.
Two different control procedures are used in CN2: one for inducing an ordered 
list of rules and the other for the unordered case.

\subsection{Inductive Logic Programming}

Hypothesis construction is the core of any ILP system and there exists many different techniques for constructing them, including: Inverse Resolution, Relative Least General Generalisations, Inverse Implication, and Inverse Entailment.

We will be using the Aleph system \cite{aleph} to experiment with inductive logic programming. This system
uses the Inverse Entailment algorithm and has also incorporated features from many of its predecessors based on other techniques.

\subsection{Bayesian classifiers}

For bayesian classifiers we tested different kinds of search algorithms to build
the network structure.

The construction of the network structured is also known as
\textit{local score metrics} and various algorithms can be employed for this optimization problem.

The experiments conducted will use genetic based search, hill climbing \cite{bayes3}, simulated annealing,
and a modified hill climbing algorithm, named K2 \cite{bayes2}.

For the second step in building a bayesian network classifier, we used a
simple estimator to estimate the conditional probability distributions.
This simple estimator produces
direct estimates of the conditional probabilities. \cite{bayes_weka}

\section{Data Format}

The ``Adult Database'' consists of 48842 entries of individual information of 
people gathered in a 1994 census on the United States of America. From the
global collection of data, a representative sample was collected from people
with ages between 16 and 100 and a final weight above 1.

The data set is divided in two groups: the train set with 32561 records and the test set with 16281 entries.

Each entry is composed by a set of 14 attributes and two classes, identifying
persons who gain more or less than 50K dollars per year. The attributes and types are:

\begin{itemize}
  \item{\textbf{Age}: continuous.}
  \item{\textbf{Workclass}: categorical: [[Private], [Self-emp-not-inc], 
        [Self-emp-inc], [Federal-gov], [Local-gov], [State-gov], [Without-pay], 
        [Never-worked]]}
  \item{\textbf{FNLWGT}: continuous. Represents the final weight. People with 
        the same demographical characteristics should have the same weight.}
  \item{\textbf{Education}: ordinal: [1. [Preschool], 2. [1st-4th], 3. 
        [5th-6th], 4. [7th-8th], 5. [9th < 10th], 6. [11th < 12th], 7. 
        [HS-grad], 8. [Prof-school], 9. [Assoc-acdm], 10. [Assoc-voc], 11.
        [Some-college], 12. [Bachelors], 13. [Masters], 14. [Doctorate]]}
  \item{\textbf{Education-num}: continuous. Is a continuous representation of 
        the \textbf{Education} attribute.}
  \item{\textbf{Marital-status}: categorical: [[Married-civ-spouse], [Divorced],
        [Never-married], [Separated], [Widowed], [Married-spouse-absent],
        [Married-AF-spouse]]}
  \item{\textbf{Occupation}: categorical: [[Tech-support], [Craft-repair],
        [Other-service], [Sales], [Exec-managerial], [Prof-specialty], 
        [Handlers-cleaners], [Machine-op-inspct], [Adm-clerical], 
        [Farming-fishing], [Transport-moving], [Priv-house-serv], 
        [Protective-serv], [Armed-Forces]]}
  \item{\textbf{Relationship}: categorical: [[Wife], [Own-child], [Husband], 
        [Not-in-family], [Other-relative], [Unmarried]]}
  \item{\textbf{Race}: categorial: [[White], [Asian-Pac-Islander], 
        [Amer-Indian-Eskimo], [Other], [Black]]}
  \item{\textbf{Sex}: categorical: [[Male], [Female]]}
  \item{\textbf{Capital-gain}: continuous.}
  \item{\textbf{Capital-loss}: continuous.}
  \item{\textbf{Hours-per-week}: continuous.}
  \item{\textbf{Native-country}: categorical.}
\end{itemize}

There are 3620 records with missing values. The attributes that contain missing values are: \textbf{Workclass}, \textbf{Occupation} and \textbf{Native-country}.

This data set is available in the following URL: \url{ftp://ftp.ics.uci.edu/pub/machine-learning-databases/adult}.

\section{Experimental Context}

In this section we will describe the experimental context for each
algorithm described in the section \ref{sec:algs}, namely, programs we used,
preprocessing tasks conducted, the parameterization that is available for each
algorithm and so forth.

The data set available in \url{ftp://ftp.ics.uci.edu/pub/machine-learning-databases/adult}
is composed of two CSV files, one for the training set and the other for the test set.
Each file was imported into a MySQL database and grouped into two tables: adult and adult\_test.
These tables follow the same structure (Table \ref{tbl:mysql_table}).

\begin{table}[ht]
  \begin{tabular}{ | l | p{14cm} |}
    \hline
    \textbf{Field} & \textbf{Type} \\ \hline
    id & SERIAL \\ \hline
    age & INT \\ \hline
    workclass & enum('private', 'self\_emp\_not\_inc', 'self\_emp\_inc','federal\_gov', 'local\_gov', 'state\_gov', 'without\_pay', 'never\_worked', 'unknown') \\ \hline
    fnlwgt & INT \\ \hline
    education & enum('bachelors', 'some\_college', '11th', 'hs\_grad', 'prof\_school', 'assoc\_acdm', 'assoc\_voc', '9th', '7th\_8th', '12th', 'masters', '1st\_4th', '10th', 'doctorate', '5th\_6th', 'preschool') \\ \hline
    education\_num & INT \\ \hline
    marital\_status & enum('married\_civ\_spouse', 'divorced', 'never\_married', 'separated', 'widowed', 'married\_spouse\_absent', 'married\_af\_spouse') \\ \hline
    occupation & enum('tech\_support', 'craft\_repair', 'other\_service', 'sales', 'exec\_managerial', 'prof\_specialty', 'handlers\_cleaners', 'machine\_op\_inspct', 'adm\_clerical', 'farming\_fishing', 'transport\_moving', 'priv\_house\_serv', 'protective\_serv', 'armed\_forces', 'unknown') \\ \hline
    relationship & enum('wife', 'own\_child', 'husband', 'not\_in\_family', 'other\_relative', 'unmarried') \\ \hline
    race & enum('white', 'asian\_pac\_islander', 'amer\_indian\_eskimo', 'other', 'black') \\ \hline
    sex & enum('male', 'female') \\ \hline
    capital\_gain & INT \\ \hline
    capital\_loss & INT \\ \hline
    hours\_per\_week & INT \\ \hline
    native\_country & enum('united\_states', 'cambodia', 'england', 'puerto\_rico', 'canada', 'germany', 'outlying\_us', 'india', 'japan', 'greece', 'south', 'china', 'cuba', 'iran', 'honduras', 'philippines', 'italy', 'poland', 'jamaica', 'vietnam', 'mexico', 'portugal', 'ireland', 'france', 'dominican\_republic', 'laos', 'ecuador', 'taiwan', 'haiti', 'columbia', 'hungary', 'guatemala', 'nicaragua', 'scotland', 'thailand', 'yugoslavia', 'el\_salvador', 'trinadad\_tobago', 'peru', 'hong', 'holand\_netherlands', 'unknown') \\ \hline
    plus\_50 & BOOLEAN \\ \hline
  \end{tabular}
  \caption{MySQL table structure.}
  \label{tbl:mysql_table}
\end{table}

For the majority of the proposed algorithms we tried to make use of the database, instead
of using the CSV files, as it allows a more flexible approach to data querying using SQL.

\subsection{Weka}

Weka is a collection of machine learning algorithms for data mining tasks. \cite{weka}
It can be applied directly to a dataset using command line tools or through Java code, using the Weka Java APIs.
Weka supports tools for data pre-processing, classification, regression, clustering, association rules,
and visualization.

This tool was used to apply the following algorithms: C4.5, ADTree, Random Forest, CART and Bayesian Networks.
Instead of using the command line tools, we used the Java API to build classifiers.

To use the data set we fetch it directly from the database and then apply various data pre-processing
techniques that are available within Weka:

\begin{itemize}
  \item The \textbf{plus\_50} was converted from a numerical format to categories: $<$50K and $>$50K.
  \item Every field marked as \textbf{unknown} was marked as \textbf{missing}.
  \item The \textbf{age} was discretized into various categories: \textbf{young} ($<$25),
    \textbf{middle\_aged} (25-45), \textbf{senior} (45-65), and \textbf{old} ($>$65).
  \item The attribute \textbf{hours\_per\_week} was also discretized: \textbf{part\_time} ($<$25),
    \textbf{full\_time} (25-40), \textbf{over\_time} (40-60), and \textbf{workaholic} ($>$60).
  \item Each capital attribute (\textbf{capital\_gain} and \textbf{capital\_loss}) was also discretized:
    \textbf{none} (0), \textbf{very\_low} (0-100), \textbf{low} (100-1000), \textbf{medium} (1000-5000),
    \textbf{high} (5000-10000), and \textbf{very\_high} ($>$10000).
\end{itemize}

Only the following values are feed into the algorithms: age, workclass, education,
marital\_status, occupation, relationship, race, sex, capital\_gain, capital\_loss,
hours\_per\_week, native\_country, and plus\_50. The attributes education\_num
and fnlwgt are derived from other attributes and so we keep them out.

For missing values we used the \textit{ReplaceMissingValues} filter available in Weka that
replaces unknown values with the modes and means from the data set. 

Each algorithm has various configuration parameters that will be explored
and analyzed in section \ref{sec:experimental_eval}.
For example, the Bayesian Network
algorithm allows the parameterization of the search algorithm that optimizes the network structure.

\section{Experimental Evaluation} \label{sec:experimental_eval}

\section{Related Work}

\section{Conclusions}

\begin{thebibliography}{1}
\bibitem{1}
Mitchel, T.; Machine Learning; McGraw-Hill, 1997
\bibitem{2}
Han, J., Kamber, M., Pei, J.; Data Mining: Concepts and Techniques, Second Edition; Morgan Kaufmann, 2005
\bibitem{3}
H. Witten, I., Frank, E.; Data Mining: Practical Machine Learning Tools and Techniques, Second Edition; Morgan Kaufmann, 2005
\bibitem{4}
RapidMiner; \url{http://www.rapidminer.com/}, November 2009
\bibitem{c45}
Ross Quinlan, J.; C4.5: Programs for Machine Learning; Morgan Kaufmann, 1992
\bibitem{6}
The R Project for Statistical Computing; http://www.r-project.org/, November 2009
\bibitem{7}
Hahsler, M., Gruen, B.; Hornik, K.; arules -- A Computational Environment for Mining Association Rules and Frequent Item Sets; Journal of Statistical Software 14/15, 2005

\bibitem{id3}
Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.

\bibitem{cart}
Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (1984). Classification and Regression Trees. Wadsworth International Group, Belmont, California.

\bibitem{random_forest}
Liaw, Andy and Wiener, Matthew. Classification and Regression by randomForest. R News (2002) Vol. 2/3 p. 18

\bibitem{adtree}
Freund, Y., Mason, L.: The alternating decision tree learning algorithm. In: Proceeding of the Sixteenth International Conference on Machine Learning, Bled, Slovenia, 124-133, 1999

\bibitem{bayes}
N. Friedman, D. Geiger, M. Goldszmidt. Bayesian Network Classifiers. Machine Learning, 29: 131–163, 1997

\bibitem{rule_induction}
Quinlan, J. R. (1987). "Generating production rules from decision trees". in McDermott, John. Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87). Milan, Italy. pp. 304–307

\bibitem{aleph}
Ashwin Srinivasan. The Aleph Manual. 2007

\bibitem{bayes2}
G. Cooper, E. Herskovits. A Bayesian method for the induction of probabilistic networks from 
data. Machine Learning, 9: 309–347, 1992.

\bibitem{bayes3}
W.L. Buntine. A guide to the literature on learning probabilistic networks from data. IEEE 
Transactions on Knowledge and Data Engineering, 8:195–210, 1996.

\bibitem{bayes_weka}
Remco R. Bouckaert. Bayesian Network Classifiers in Weka. 2004

\bibitem{weka}
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten (2009); The WEKA Data Mining Software: An Update; SIGKDD Explorations, Volume 11, Issue 1.

\end{thebibliography}

\clearpage

\appendix

\section{Tables}


\end{document}