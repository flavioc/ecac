\documentclass[a4paper]{llncs}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[utf8]{inputenc}
\usepackage{subfigure}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{multirow}
\usepackage[hyphens]{url}
\urldef{\mails}\path|{ei05011,ei05028}@fe.up.pt|
\bibliographystyle{splncs}
\begin{document}
\title{A comparison of classification algorithms applied to the Adult Database}
\author{Flávio Cruz \and João Azevedo}
\institute{Faculdade de Engenharia da Universidade do Porto\\
    Rua Dr. Roberto Frias, s/n 4200-465 Porto PORTUGAL\\
    \mails}
\maketitle

\begin{abstract}
Highly frequent in experimental tests related to data mining, the Adult Database
has some interesting properties which make it quite useful to test the 
efficiency of classification algorithms. This report aims to describe the Adult
Database from a statistical perspective and taking into account its support for
data mining tasks.
\end{abstract}

\section{Introduction}

\subsection{Problem Identification}

Deducing from the previous section, this is a classification problem. Given an adult
with the mentioned attributes we want to classify him into one of the following groups:
one gains more than 50000 dollars per year, the other group does not.

One important objective in classifying things is to understand why the given object
is put under one class and not in another \cite{2}\cite{3}, that is, we want to understand what social characteristics
allows one person to gain more than someone else, if the sex is important, the education level,
the race and what factors are less important.

Given a specific set of persons from a certain society and from a certain time we could then
learn what contributes to a person's social standing and, as we all know, income is an important
metric in that department. We could, for example, learn if the race affects the persons success and if
that society discriminates against certain races.

Understanding what affects the income can give a lot of information concerning society's behaviors.

\section{Algorithms} \label{sec:algs}

The algorithms we used to build classifiers can be grouped into four main
groups: decision trees, rule induction \cite{rule_induction},
inductive logic programming and bayesian classifiers \cite{bayes}.

\subsection{Decision trees}

For decision trees, the algorithms used are ID3 \cite{id3}, C4.5 \cite{c45}, CART \cite{cart},
Random Forest \cite{random_forest}, and alternating decision trees (adtree) \cite{adtree}.

Decision tree learning is a very commonly used method in classification. The main goal here is to
create a tree model that can predict an object's class. Each interior node corresponds
to one test for the object's attributes and each edge links to new interiors nodes based on the corresponding
results. Each leaf in the tree represents the object class for the tests made
across the respective path, from root to leaf.

The ID3 algorithm (Iterative Dichotomiser 3) was one of the first decision trees algorithms.
It was designed by Ross Quinlan.

The C4.5 algorithm is the improved version of the ID3 algorithm, also designed by Quinlan.
Contrary to ID3, this algorithm can handle both continuous and discrete attributes,
it can use data with missing values and can also prune its decision trees.

The CART algorithm builds classification and regression trees for predicting continuous dependent variables (regression)
and categorical predictor variables (classification). The classic CART algorithm was popularized by Breiman et al. \cite{cart}

The alternating decision tree is another type of decision tree algorithms. This tree
consists of decision nodes, which specify a predicate condition, and prediction nodes, containing a single number.
Each object is classified by following all paths for which all decision nodes are true and summing any
prediction nodes along the way. Contrary to binary classification like ID3 ou C4.5, in adtrees one
object can follow multiple paths. 

Finally, the Random Forest algorithm is an ensemble classifier which consists of many decision trees.
Each object is classified multiple times by all decision trees and the mode class is used as the final class.
Like the CART algorithm, Random Forest was also developed by Leo Breiman.

\subsection{Rule induction}

To experiment with rule induction algorithms we chose the CN2 \cite{cn2} algorithm.

Rule induction works by extracting formal rules from a set of observations.
Usually rules are expressed in the form \textit{if condition1 and condition2 ... then (decision, value)}.

CN2 is an algorithm for inducing 
propositional classification rules. CN2 consists of two main procedures: the 
search procedure that performs beam search in order to find a single rule and 
the control procedure that repeatedly executes the search. 
The search procedure performs beam search using classification accuracy 
of the rule as a heuristic function.
Two different control procedures are used in CN2: one for inducing an ordered 
list of rules and the other for the unordered case.

\subsection{Inductive Logic Programming}

Hypothesis construction is the core of any ILP system and there exists many different techniques for constructing them, including: Inverse Resolution, Relative Least General Generalisations, Inverse Implication, and Inverse Entailment.

We will be using the Aleph system \cite{aleph} to experiment with inductive logic programming. This system
uses the Inverse Entailment algorithm and has also incorporated features from many of its predecessors based on other techniques.

\subsection{Bayesian classifiers}

For bayesian classifiers we tested different kinds of search algorithms to build
the network structure.

The construction of the network structured is also known as
\textit{local score metrics} and various algorithms can be employed for this optimization problem.

The experiments conducted will use genetic based search, hill climbing \cite{bayes3}, and simulated annealing.

For the second step in building a bayesian network classifier, we used a
simple estimator to estimate the conditional probability distributions.
This simple estimator produces
direct estimates of the conditional probabilities. \cite{bayes_weka}

\section{Data Format}

The ``Adult Database'' consists of 48842 entries of individual information of 
people gathered in a 1994 census on the United States of America. From the
global collection of data, a representative sample was collected from people
with ages between 16 and 100 and a final weight above 1.

The data set is divided in two groups: the train set with 32561 records and the test set with 16281 entries.

Each entry is composed by a set of 14 attributes and two classes, identifying
persons who gain more or less than 50K dollars per year. The attributes and types are:

\begin{itemize}
  \item{\textbf{Age}: continuous.}
  \item{\textbf{Workclass}: categorical: [[Private], [Self-emp-not-inc], 
        [Self-emp-inc], [Federal-gov], [Local-gov], [State-gov], [Without-pay], 
        [Never-worked]]}
  \item{\textbf{FNLWGT}: continuous. Represents the final weight. People with 
        the same demographical characteristics should have the same weight.}
  \item{\textbf{Education}: ordinal: [1. [Preschool], 2. [1st-4th], 3. 
        [5th-6th], 4. [7th-8th], 5. [9th < 10th], 6. [11th < 12th], 7. 
        [HS-grad], 8. [Prof-school], 9. [Assoc-acdm], 10. [Assoc-voc], 11.
        [Some-college], 12. [Bachelors], 13. [Masters], 14. [Doctorate]]}
  \item{\textbf{Education-num}: continuous. Is a continuous representation of 
        the \textbf{Education} attribute.}
  \item{\textbf{Marital-status}: categorical: [[Married-civ-spouse], [Divorced],
        [Never-married], [Separated], [Widowed], [Married-spouse-absent],
        [Married-AF-spouse]]}
  \item{\textbf{Occupation}: categorical: [[Tech-support], [Craft-repair],
        [Other-service], [Sales], [Exec-managerial], [Prof-specialty], 
        [Handlers-cleaners], [Machine-op-inspct], [Adm-clerical], 
        [Farming-fishing], [Transport-moving], [Priv-house-serv], 
        [Protective-serv], [Armed-Forces]]}
  \item{\textbf{Relationship}: categorical: [[Wife], [Own-child], [Husband], 
        [Not-in-family], [Other-relative], [Unmarried]]}
  \item{\textbf{Race}: categorial: [[White], [Asian-Pac-Islander], 
        [Amer-Indian-Eskimo], [Other], [Black]]}
  \item{\textbf{Sex}: categorical: [[Male], [Female]]}
  \item{\textbf{Capital-gain}: continuous.}
  \item{\textbf{Capital-loss}: continuous.}
  \item{\textbf{Hours-per-week}: continuous.}
  \item{\textbf{Native-country}: categorical.}
\end{itemize}

There are 3620 records with missing values. The attributes that contain missing values are: \textbf{Workclass}, \textbf{Occupation} and \textbf{Native-country}.

This data set is available in the following URL: \url{ftp://ftp.ics.uci.edu/pub/machine-learning-databases/adult}.

\section{Experimental Context}

In this section we will describe the experimental context for each
algorithm described in the section \ref{sec:algs}, namely, programs we used,
preprocessing tasks conducted, the parameterization that is available for each
algorithm and so forth.

The data set available in \url{ftp://ftp.ics.uci.edu/pub/machine-learning-databases/adult}
is composed of two CSV files, one for the training set and the other for the test set.
Each file was imported into a MySQL database and grouped into two tables: adult and adult\_test.
These tables follow the same structure (Table \ref{tbl:mysql_table}).

\begin{table}[ht]
  \begin{tabular}{ | l | p{14cm} |}
    \hline
    \textbf{Field} & \textbf{Type} \\ \hline
    id & SERIAL \\ \hline
    age & INT \\ \hline
    workclass & enum('private', 'self\_emp\_not\_inc', 'self\_emp\_inc','federal\_gov', 'local\_gov', 'state\_gov', 'without\_pay', 'never\_worked', 'unknown') \\ \hline
    fnlwgt & INT \\ \hline
    education & enum('bachelors', 'some\_college', '11th', 'hs\_grad', 'prof\_school', 'assoc\_acdm', 'assoc\_voc', '9th', '7th\_8th', '12th', 'masters', '1st\_4th', '10th', 'doctorate', '5th\_6th', 'preschool') \\ \hline
    education\_num & INT \\ \hline
    marital\_status & enum('married\_civ\_spouse', 'divorced', 'never\_married', 'separated', 'widowed', 'married\_spouse\_absent', 'married\_af\_spouse') \\ \hline
    occupation & enum('tech\_support', 'craft\_repair', 'other\_service', 'sales', 'exec\_managerial', 'prof\_specialty', 'handlers\_cleaners', 'machine\_op\_inspct', 'adm\_clerical', 'farming\_fishing', 'transport\_moving', 'priv\_house\_serv', 'protective\_serv', 'armed\_forces', 'unknown') \\ \hline
    relationship & enum('wife', 'own\_child', 'husband', 'not\_in\_family', 'other\_relative', 'unmarried') \\ \hline
    race & enum('white', 'asian\_pac\_islander', 'amer\_indian\_eskimo', 'other', 'black') \\ \hline
    sex & enum('male', 'female') \\ \hline
    capital\_gain & INT \\ \hline
    capital\_loss & INT \\ \hline
    hours\_per\_week & INT \\ \hline
    native\_country & enum('united\_states', 'cambodia', 'england', 'puerto\_rico', 'canada', 'germany', 'outlying\_us', 'india', 'japan', 'greece', 'south', 'china', 'cuba', 'iran', 'honduras', 'philippines', 'italy', 'poland', 'jamaica', 'vietnam', 'mexico', 'portugal', 'ireland', 'france', 'dominican\_republic', 'laos', 'ecuador', 'taiwan', 'haiti', 'columbia', 'hungary', 'guatemala', 'nicaragua', 'scotland', 'thailand', 'yugoslavia', 'el\_salvador', 'trinadad\_tobago', 'peru', 'hong', 'holand\_netherlands', 'unknown') \\ \hline
    plus\_50 & BOOLEAN \\ \hline
  \end{tabular}
  \caption{MySQL table structure.}
  \label{tbl:mysql_table}
\end{table}

For the majority of the proposed algorithms we tried to make use of the database, instead
of using the CSV files, as it allows a more flexible approach to data querying using SQL.

\subsection{Weka}
\label{sec:weka}

Weka is a collection of machine learning algorithms for data mining tasks. \cite{weka}
It can be applied directly to a dataset using command line tools or through Java code, using the Weka Java APIs.
Weka supports tools for data pre-processing, classification, regression, clustering, association rules,
and visualization.

This tool was used to apply the following algorithms: C4.5, ADTree, Random Forest, CART and Bayesian Networks.
Instead of using the command line tools, we used the Java API to build classifiers.

To use the data set we fetch it directly from the database and then apply various data pre-processing
techniques that are available within Weka:

\begin{itemize}
  \item The \textbf{plus\_50} was converted from a numerical format to categories: $<$50K and $>$50K.
  \item Every field marked as \textbf{unknown} was marked as \textbf{missing}.
  \item The \textbf{age} was discretized into various categories: \textbf{young} ($<$25),
    \textbf{middle\_aged} (25-45), \textbf{senior} (45-65), and \textbf{old} ($>$65).
  \item The attribute \textbf{hours\_per\_week} was also discretized: \textbf{part\_time} ($<$25),
    \textbf{full\_time} (25-40), \textbf{over\_time} (40-60), and \textbf{workaholic} ($>$60).
  \item Each capital attribute (\textbf{capital\_gain} and \textbf{capital\_loss}) was also discretized:
    \textbf{none} (0), \textbf{very\_low} (0-100), \textbf{low} (100-1000), \textbf{medium} (1000-5000),
    \textbf{high} (5000-10000), and \textbf{very\_high} ($>$10000).
\end{itemize}

Only the following values are feed into the algorithms: age, workclass, education,
marital\_status, occupation, relationship, race, sex, capital\_gain, capital\_loss,
hours\_per\_week, native\_country, and plus\_50. The attributes education\_num
and fnlwgt are derived from other attributes and so we keep them out.

For missing values we used the \textit{ReplaceMissingValues} filter available in Weka that
replaces unknown values with the modes and means from the data set. 

Each algorithm has various configuration parameters that will be explored
and analyzed in section \ref{sec:experimental_eval}.
For example, the Bayesian Network
algorithm allows the parameterization of the search algorithm that optimizes the network structure.

After the data as been pre-processed,
we will build the classifier using the train data set.
Once the classifier is trained, it will be used to classify both the train and test data.

Performance metrics for both classification results will be registered: error rate and time spent
building the classifier and classifying all the examples.

\subsection{RapidMiner}

RapidMiner is an environment for machine learning and data mining experiments 
\cite{4}. Experiments are done by composing a process made up of a large amount
of nestable operators. This kind of approach makes it very suitable to create
and modify different kinds of experiments in a quick fashion.

RapidMiner provides both a GUI to design the process pipeline and a command-line
tool to load XML-based configuration files. When using the GUI, users can easily
switch between the pipeline visualization and the corresponding XML file,
allowing for a better understanding of the configuration file internals. A large
number of different outputs can be configured on a RapidMiner's process 
pipeline, providing a broad range of visualizations for the final results.
RapidMiner also integrates learning schemes and attribute evaluators from the 
Weka \cite{weka} learning environment (see section \ref{sec:weka} for more 
details) and provides a programming API for use in different environments.

This tool was used to apply the ID3 \cite{id3}\cite{1} algorithm, mainly because
of its support for numerical attributes (not seen in Weka), which enabled a
better comparison of approaches.

The data pre-processing conducted consisted on a missing value replacement by 
the average on numerical values and by the mode on the nominal/ordinal values. 
An absolute discretization was applied to the numerical values for testing
purposes of the default ID3 implementation.

\subsection{CN2}

CN2 \cite{cn2} is a learning algorithm for rule induction \cite{rule_induction}
that isn't available in Weka nor in RapidMiner. CN2 inductively learns a set of
propositional if...then.. rules from a set of training examples, performing a
general-to-specific beam search through the rule space for the "best" rule, 
removing training examples covered by it, and repeating it until no more "good"
rules can be found. The original version of the algorithm used entropy and
significance test to decide the best rule. Later, it was improved to use the
Laplace estimate \cite{cn2_laplace}, which is implemented in Peter Clark's 
software used for our testing purposes.

Peter Clark's CN2 software is built in C and provides a command-line environment
for conducting experiments. The input format is limited to two files, one
defining the attributes and other defining the training examples. The software 
also provides an environment for performance evaluation of the produced set of 
rules, by accepting a test file with examples.

Only the following values are feed into the algorithms: age, workclass, 
education, marital\_status, occupation, relationship, race, sex, capital\_gain,
capital\_loss, hours\_per\_week, native\_country, and plus\_50. The attributes
education\_num and fnlwgt are derived from other attributes and so are kept out.
In terms of data pre-processing for missing values, continuous values were 
replaced by the average value, while discrete values were replaced by the mode.

In terms of performance metrics, we took into account the error rate and the 
time spent building the rule set. 

\subsection{Aleph}

Aleph \cite{aleph} is currently one of the most widely used ILP \cite{ilp} 
systems. Developed by Ashwin Srinivasan is, as of 2007, in its 5th version. The
basic Aleph algorithm (used on our experiments) consists of 4 steps:

\begin{enumerate}
    \item{Select example}
    \item{Build most-specific clause}
    \item{Search}
    \item{Remove redundant}
\end{enumerate}

Aleph uses inverse entailment to build the most-specific clause and its basic
algorithm uses a best-first search at the search step. Advanced use of Aleph 
allows tweaks in every step of the algorithm, namely:

\begin{itemize}
    \item{\textbf{Select example}: The size of the sample can be configured.}
    \item{\textbf{Build most-specific clause}: The approach on building the 
    bottom-clause can be configured. The evaluation mechanism can also be 
    tweaked.}
    \item{\textbf{Search}: A huge range of search strategies, evaluation 
    functions and refinement operators can be applied.}
    \item{\textbf{Remove redundant}: A strategy of retaining covered examples
    can be applied, for a better estimation of clause scores.}
\end{itemize}

Aleph is built entirely in prolog and is best suited to run on the YAP Prolog 
engine. 

In order to use ILP to build a classifier for our data set, some data
pre-processing needs to be run, namely on converting the examples to logic 
predicates. Despite the fact that Aleph is able to deal with numerical values,
discretization has been run on the variables ``age'', ``capital\_gain'', 
``capital\_loss'' and ``hours\_per\_week''. The value distribution was the same
as described in section \ref{sec:weka}. The main classification clause is given
by \verb$more_than_50K(+person).$ Those persons winning more than 50K per year
are the positive examples, the rest are the negative examples. Each of the
attributes was converted to unary prolog predicates (for example 
\verb$young(+person).$, \verb$work_private(+person).$, \verb$male(+person).$,
\verb$is_native_of_portugal(+person).$). Missing values were simply ommited from
the background knowledge.

In terms of performance metrics, we took into account the error rate and the 
time spent building the hypothesis set. The expressiveness of the results is
also analyzed in section \ref{sec:experimental_eval}.

\section{Experimental Evaluation} \label{sec:experimental_eval}

This section shows performance results for the various algorithms described in section \ref{sec:algs}.

\subsection{ID3}

ID3 in RapidMiner is implemented in two different flavours, one that supports
numerical values, one that doesn't. For the one that doesn't support numerical
values, an absolute discretization was conducted. Table \ref{tbl:results_id3}
summarizes the results with the ID3 algorithm.

\begin{table}
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | c |}
    \hline
    \textbf{Type} & \textbf{Criterion} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} \\ \hline
    ID3 & Gain Ratio & 0 & 0 & 0 \\ \hline
    ID3 & Information Gain & 0 & 0 & 0 \\ \hline
    ID3 & Gini Index & 0 & 0 & 0 \\ \hline
    ID3 & Accuracy & 0 & 0 & 0 \\ \hline
    ID3 Numerical & Gain Ratio & 0 & 0 & 0 \\ \hline
    ID3 Numerical & Information Gain & 0 & 0 & 0 \\ \hline
    ID3 Numerical & Gini Index & 0 & 0 & 0 \\ \hline
    ID3 Numerical & Accuracy & 0 & 0 & 0 \\ \hline
  \end{tabular}
  \caption{Results for ID3.}
  \label{tbl:results_id3}
  \end{center}
\end{table}

\subsection{C4.5}

C4.5 in Weka provides a number of parameters to tune the algorithm.
One of these parameters allows the creation of unpruned trees $<$\textbf{-U}$>$.
For pruned trees we can use the confidence threshold parameter that dictates how
much pruning the algorithm will do $<$\textbf{-C float}$>$.

We executed the algorithm using different combinations of the parameters described above.
Table \ref{tbl:results_c45} displays the obtained results. From the table
we note that once the confidence threshold reaches a certain level ($\approx$0.55) the results
are very similar for the unpruned classifier and the over-fitting effect starts to get worse,
where the train data gets very good results ($\approx$11.19 error rate) and the test set gets a
large error rate ($\approx$24).

The confidence threshold for best test error rates appears to be in the interval 0.05-0.15.   

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} & \textbf{Nodes} & \textbf{Leaves} \\ \hline
    -U & 11.18 & 23.74 & 0m24.378s & 6919 & 6198 \\ \hline
    -C 0.05 & 15.67 & 22.86 & 0m24.414s & 46 & 38 \\ \hline
    -C 0.1 & 15.22 & 21.69 & 0m25.417s & 106 & 89 \\ \hline
    -C 0.15 & 14.85 & 22.25 & 0m25.086s & 186 & 158 \\ \hline
    -C 0.2 & 14.52 & 22.05 & 0m24.436s & 298 & 254 \\ \hline
    -C 0.25 & 14.40 & 22.08 & 0m24.628s & 376 & 319 \\ \hline
    -C 0.3 & 14.17 & 23.26 & 0m24.765s & 494 & 423 \\ \hline
    -C 0.35 & 13.84 & 22.42 & 0m25.154s & 692 & 593 \\ \hline
    -C 0.4 & 13.68 & 22.49 & 0m24.325s & 835 & 721 \\ \hline
    -C 0.45 & 13.26 & 22.43 & 0m25.086s & 1143 & 992 \\ \hline
    -C 0.5 & 12.95 & 22.51 & 0m24.874s & 1483 & 1292 \\ \hline
    -C 0.55 & 11.19 & 24.94 & 0m25.899s & 6727 & 6036 \\ \hline
    -C 0.6 & 11.19 & 23.94 & 0m26.882s & 6727 & 6036 \\ \hline
    -C 0.7 & 11.19 & 23.94 & 0m26.638s & 6727 & 6036 \\ \hline
    -C 0.8 & 11.19 & 23.94 & 0m26.818s & 6727 & 6036 \\ \hline
    -C 0.9 & 11.19 & 23.94 & 0m26.634s & 6727 & 6036 \\ \hline
  \end{tabular}
  \caption{Results for C4.5.}
  \label{tbl:results_c45}
  \end{center}
\end{table}

A decision tree built by this algorithm with the confidence threshold set to 0.05 is shown in Listing \ref{decision_tree_c45}.

\subsection{Alternating Decision Trees}

Alternating Decision Trees are related to a concept named \textit{boosting}.
Boosting is based on the observation that finding many rough rules of thumb can be a lot easier 
than finding a single, highly accurate prediction rule. To apply the boosting approach,
we start with a method or algorithm for finding the rough rules of thumb. 
The boosting algorithm calls this “weak” or “base” learning algorithm repeatedly, 
each time feeding it a different subset of the training examples. Each time 
it is called, the base learning algorithm generates a new weak prediction rule, and 
after many rounds, the boosting algorithm must combine these weak rules into a 
single prediction rule that, hopefully, will be much more accurate than any one of 
the weak rules. \cite{boosting}

The ADTree's Weka algorithm can be parametrized by the number of boosting iterations
\textbf{$<$-B integer$>$} and by the way the algorithm will expand the nodes,
\textbf{$<$-E [-3,-2,-1,$>=$0]$>$},
-3 for \textit{all}, -2 for \textit{weight} based expanding,
-1 for \textit{z\_pure} and a number
equal or greater than zero as a seed for random walks.

We experimented the algorithm using various combinations of those two parameters.

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} & \textbf{Nodes} & \textbf{Leaves} \\ \hline
    -E -3 -B 2 & 21.80 & 21.44 & 0m24.641s & 7 & 5 \\ \hline
    -E -3 -B 5 & 18.20 & 20.99 & 0m24.571s & 16 & 11 \\ \hline
    -E -3 -B 10 & 16.13 & 20.40 & 0m26.539s & 31 & 21 \\ \hline
    -E -3 -B 15 & 15.91 & 20.19 & 0m28.565s & 46 & 31 \\ \hline
    -E -3 -B 20 & 15.25 & 20.39 & 0m32.336s & 61 & 41 \\ \hline
    -E -3 -B 25 & 15.08 & 20.37 & 0m37.223s & 76 & 51 \\ \hline
    -E -3 -B 30 & 14.88 & 20.25 & 0m41.062s & 91 & 61 \\ \hline
    -E -3 -B 35 & 14.78 & 20.01 & 0m45.760s & 106 & 71 \\ \hline
    -E -3 -B 40 & 14.68 & 20.58 & 0m52.976s & 121 & 81 \\ \hline
    
    -E -2 -B 2 & 21.80 & 21.44 & 0m25.219s & 7 & 5 \\ \hline
    -E -2 -B 5 & 18.20 & 20.99 & 0m24.445s & 16 & 11 \\ \hline
    -E -2 -B 10 & 16.34 & 20.71 & 0m25.390s & 31 & 21 \\ \hline
    -E -2 -B 15 & 15.80 & 20.42 & 0m26.982s & 46 & 31 \\ \hline
    -E -2 -B 20 & 15.13 & 19.80 & 0m29.050s & 58 & 39 \\ \hline
    -E -2 -B 25 & 14.95 & 20.25 & 0m28.956s & 73 & 49 \\ \hline
    -E -2 -B 30 & 14.93 & 20.07 & 0m31.118s & 88 & 59 \\ \hline
    -E -2 -B 35 & 14.90 & 20.20 & 0m32.565s & 100 & 67 \\ \hline
    -E -2 -B 40 & 14.65 & 20.73 & 0m34.003s & 115 & 77 \\ \hline
    
    -E -1 -B 2 & 21.80 & 21.436 & 0m24.149s & 7 & 5 \\ \hline
    -E -1 -B 5 & 18.20 & 20.99 & 0m24.061s & 16 & 11 \\ \hline
    -E -1 -B 10 & 16.34 & 20.71 & 0m24.967s & 31 & 21 \\ \hline
    -E -1 -B 15 & 15.80 & 20.42 & 0m27.095s & 46 & 31 \\ \hline
    -E -1 -B 20 & 15.13 & 19.80 & 0m28.813s & 58 & 38 \\ \hline
    -E -1 -B 25 & 14.95 & 20.25 & 0m29.205s & 73 & 49 \\ \hline
    -E -1 -B 30 & 14.93 & 20.07 & 0m30.569s & 88 & 59 \\ \hline
    -E -1 -B 35 & 14.93 & 20.20 & 0m32.708s & 100 & 67 \\ \hline
    -E -1 -B 40 & 14.65 & 20.73 & 0m33.205s & 115 & 77 \\ \hline
    
    -E 0 -B 2 & 21.80 & 21.44 & 0m23.717s & 7 & 4 \\ \hline
    -E 0 -B 5 & 18.21 & 20.99 & 0m24.619s & 16 & 11 \\ \hline
    -E 0 -B 10 & 16.37 & 20.71 & 0m24.512s & 31 & 21 \\ \hline
    -E 0 -B 15 & 15.81 & 20.42 & 0m25.240s & 46 & 31 \\ \hline
    -E 0 -B 20 & 14.94 & 20.07 & 0m25.589s & 61 & 41 \\ \hline
    -E 0 -B 25 & 15.04 & 20.15 & 0m25.868s & 76 & 51 \\ \hline
    -E 0 -B 30 & 15.01 & 20.50 & 0m26.492s & 91 & 61 \\ \hline
    -E 0 -B 35 & 14.96 & 20.53 & 0m26.294s & 106 & 71 \\ \hline
    -E 0 -B 40 & 14.90 & 20.54 & 0m27.248s & 118 & 79 \\ \hline
    
  \end{tabular}
  \caption{Results for ADTree.}
  \label{tbl:results_adtree}
  \end{center}
\end{table}

From Table \ref{tbl:results_adtree} we can see that the number of boosting iterations affects
both the execution time (directly proportional) and the error rate.
The best error rate is for the parameters \textbf{-E -2 -B 20} giving $\approx$19.80\% for the test set.

Another good observation is that this algorithm can build small trees that give satisfactory results, both for
the train set and for the test set.

\subsection{Random Forest}

In Weka, the Random Forest algorithm can be parametrized by the number
of trees to build \textbf{$<$-I integer$>$}. Table \ref{tbl:results_random_forest} shows results for
a varying number of trees.

From the table we conclude that we need a certain amount of trees ($\approx$80) to
reach a satisfactory error rate ($<$22\%) for the test set. Interestingly
the error rate for the train set is low ($\approx$9\%), which indicates over-fitting
when comparing the difference to the test error rate.

One drawback of using a big number of trees is the use of large
quantities of memory (we needed 1GB to compute 400 trees) and the long time needed to construct the classifier.

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} \\ \hline
    -I 2 & 10.52 & 34.03 & 0m25.861s \\ \hline
    -I 3 & 9.83 & 34.00 & 0m26.318s \\ \hline
    -I 4 & 9.69 & 30.91 & 0m26.154s \\ \hline
    -I 5 & 9.50 & 29.62 & 0m27.108s \\ \hline
    -I 7 & 9.32 & 28.03 & 0m27.158s \\ \hline
    -I 10 & 9.24 & 29.80 & 0m28.107s \\ \hline
    -I 15 & 9.18 & 28.80 & 0m30.716s \\ \hline
    -I 20 & 9.15 & 28.09 & 0m36.340s \\ \hline
    -I 40 & 9.15 & 23.43 & 0m39.687s \\ \hline
    -I 60 & 9.15 & 23.65 & 0m49.443s \\ \hline
    -I 80 & 9.15 & 22.92 & 0m57.652s \\ \hline
    -I 100 & 9.15 & 22.83 & 1m28.024s \\ \hline
    -I 400 & 9.15 & 22.71 & 4m58.842s \\ \hline
  \end{tabular}
  \caption{Results for Random Forest.}
  \label{tbl:results_random_forest}
  \end{center}
\end{table}

\subsection{CART}

The simplified version of the CART algorithm is available
in the \textit{SimpleCart} Weka class.

This algorithm allows the disabling of various optimizations
and heuristics \textbf{$<$-U and -H $>$} as explained by the Weka manual.
We will keep these optimizations in.

Other interesting parameters that we will be changing, include:

\begin{itemize}
  \item \textbf{$<$-M int$>$}: to set the minimal number of instances at the terminal nodes.
  \item \textbf{$<$-N int$>$}: to set the number of folds used in the minimal cost-complexity pruning.
\end{itemize}

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} & \textbf{Nodes} & \textbf{Leaves} \\ \hline
    -M 2 -N 5 & 13.96 & 25.37 & 3m6.315s & 119 & 60 \\ \hline
    -M 2 -N 10 & 13.85 & 25.38 & 5m36.925s & 139 & 70 \\ \hline
    -M 2 -N 15 & 13.93 & 25.41 & 8m1.214s & 123 & 62 \\ \hline
    -M 2 -N 20 & 13.88 & 25.39 & 10m27.982s & 133 & 67 \\ \hline
    
    -M 3 -N 5 & 13.96 & 25.37 & 2m51.491s & 119 & 60 \\ \hline
    -M 3 -N 10 & 13.85 & 25.39 & 5m18.642s & 139 & 70 \\ \hline
    -M 3 -N 15 & 13.94 & 25.41 & 7m53.984s & 123 & 62 \\ \hline
    -M 3 -N 20 & 13.88 & 25.39 & 10m19.665s & 133 & 67 \\ \hline
    
    -M 5 -N 5 & 13.94 & 25.41 & 2m43.363s & 123 & 62 \\ \hline
    -M 5 -N 10 & 13.82 & 25.34 & 4m54.547s & 145 & 73 \\ \hline
    -M 5 -N 15 & 13.94 & 25.41 & 7m9.537s & 123 & 62 \\ \hline
    -M 5 -N 20 & 13.88 & 25.39 & 9m18.095s & 133 & 67 \\ \hline
  \end{tabular}
  \caption{Results for CART.}
  \label{tbl:results_cart}
  \end{center}
\end{table}

CART results are displayed in Table \ref{tbl:results_cart}.
From these results we can see that there isn't a lot of variation in the results.
The algorithm also performs badly when compared to C4.5 or ADTree.

\subsection{CN2}

The CN2 software used supports different algorithms for building either ordered
or unordered rule sets and with different error estimations: laplacian, naive,
information gain and modified entropy. We've tested each combination of
parameters and the overall results are presented in table \ref{tbl:results_cn2}.

\begin{table}
\begin{center}
\begin{tabular}{ | l | l | l | l | l |}
    \hline
    \textbf{Algorithm} & \textbf{Error estimate} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} \\ \hline
    Unordered & Laplacian & 13.3 & 15.2 & 3m29s \\ \hline
    Unordered & Naive & - & - & - \\ \hline
    Ordered & Laplacian & 0 & 17.6 & 1m45s \\ \hline
    Ordered & Naive & 0 & 19.9 & 12m21s \\ \hline
    Ordered & Information Gain & 18.9 & 18.9 & 0m5s \\ \hline
    Ordered & Modified Entropy & 18.9 & 19.1 & 0m20s \\ \hline
\end{tabular}
\caption{Results for CN2.}
\label{tbl:results_cn2}
\end{center}
\end{table}

The information gain and modified entropy approaches to error estimations are
only available for the algorithm producing ordered set of rules. The approach
building an unordered set of rules with a naive error estimation lead to a stack
overflow error in the application. One thing one can note is that the process of
building classification rules takes, in average, longer than the one of building
classification trees. Nevertheless, the results are arguably more expressive and
the error rates are low (the best being 15.2\% for a previously unkown set of 
examples). Also note that the variant of building an ordered set of rules with
an information gain error estimate produces rules faster and with an acceptable
error rate (18.9\%). To analyze the expressiveness of the rules produced, a set
of rules that cover the most examples of the training data are described in 
listing \ref{cn2_rules} (using the ordered algorithm with the laplacian error 
estimation).

\subsection{ILP}

There are a lot of techniques for building ILP classifiers. Due to time
constraints (as building an ILP classifier is usually a very time-consuming 
task), we only tested the basic Aleph system, which uses inverse entailment for
building the bottom clause and a best-first search method to find the best
hypothesis. The results obtained are described in table \ref{tbl:results_ilp}.

\begin{table}
\begin{center}
\begin{tabular}{ | c | c | c | c | c |}
\hline
\vspace{1pt} & \multicolumn{4}{| c |}{\textbf{Actual}} \\ \hline
\multirow{3}{*}{\textbf{Predicted}} & \vspace{1pt} & \textbf{+} & \textbf{-} & \textbf{Total} \\ \hline
& \textbf{+} & 647 & 133 & 780 \\ \hline
& \textbf{-} & 3199 & 12302 & 15501 \\ \hline
& \textbf{Total} & 3846 & 12435 & 16281 \\ \hline
\end{tabular}
\caption{Results for ILP (Aleph).}
\label{tbl:results_ilp}
\end{center}
\end{table}

The error rate produced by Aleph when evaluating the test data was 20.45\%.
18881882 clauses were built, taking around 5 hours for the whole training
data. Despite the number of clauses, most of them are irrelevant, directly
classifying examples where no more general rule is available. Listing 
\ref{ilp_clauses} lists the most relevant clauses produced.

\subsection{Bayesian Networks}

Our strategy for bayesian networks revolve around changing the search algorithm
and its parameters. Weka easily allows the parameterization of this algorithm (\textbf{$<$-Q$>$})
that optimizes the network structure.

The search algorithms experimented include:

\begin{itemize}
  \item Genetic based search (\textit{weka.classifiers.bayes.net.search.local.GeneticSearch}) \\
  The following options are available:
  \begin{itemize}
    \item -L integer: Population size. 
    \item -A integer: Descendant population size.
    \item -U integer: Number of runs/generations.
    \item -M: Use mutation (true by default).
    \item -C: Use cross-over (true by default).
  \end{itemize}
  
  \item Hill climbing (\textit{weka.classifiers.bayes.net.search.local.HillClimber}) \\
  The following options are available:
  \begin{itemize}
    \item -P $<$nr of parents$>$: Maximum number of parents.
    \item -R: Use arc reversal operation. (default false)
  \end{itemize}
  
  \item Simulated annealing (\textit{weka.classifiers.bayes.net.search.local.SimulatedAnnealing}) \\
  The following options are available:
  \begin{itemize}
    \item -A float: Start temperature.
    \item -U integer: Number of runs.
    \item -D float: Delta temperature.
  \end{itemize}
\end{itemize}

Weka also allows the parameterization of the estimator algorithm (\textbf{$<$-E$>$}).
We will use the \textit{SimpleEstimator} estimator (\textit{weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 1.0})
for all the experiments.

\subsubsection{Genetic Search}

The Genetic Search algorithm is a genetic based algorithm to optimize the bayesian network structure.
We can configure the population size and number of generations. Mutation and cross-over can also
be set or kept out.

For this experiment we will only use a portion of the train data to speed up the process.

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | c | c |}
    \hline
    \textbf{Parameter} & \textbf{Portion} & \textbf{Portion error rate} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} \\ \hline
    GeneticSearch -C -M -L 3 -A 3 -U 3 & 5 & & & & \\ \hline
    GeneticSearch -C -M -L 3 -A 3 -U 5 & 5 & & & & \\ \hline
    GeneticSearch -C -M -L 3 -A 3 -U 15 & 5 & & & & \\ \hline
  \end{tabular}
  \caption{Results for Bayesian Networks / Genetic Search.}
  \label{tbl:results_bayesian_networks_gs}
  \end{center}
\end{table}

\subsubsection{Hill Climbing}

The best configuration for hill climbing (Table \ref{tbl:results_bayesian_networks_hc})
is when the number of parents is exactly 1, still, the error rate is somewhat unsatisfactory when
comparing to other results.

Incrementing the number of parents forces the classifier to over-fit the train set, as it
directly increases the test set error rate while decreasing the train set error rate.

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} \\ \hline
    HillClimber -P 1 & 19.22 & 22.93 & 0m29.512s \\ \hline
    HillClimber -P 2 & 15.15 & 26.34 & 0m30.340s \\ \hline
    HillClimber -P 3 & 14.51 & 28.11 & 0m29.963s \\ \hline
    HillClimber -P 4 & 14.43 & 27.92 & 0m29.945s \\ \hline
    HillClimber -P 10 & 14.43 & 27.92 & 0m30.638s \\ \hline
  \end{tabular}
  \caption{Results for Bayesian Networks / Hill Climbing.}
  \label{tbl:results_bayesian_networks_hc}
  \end{center}
\end{table}

\subsubsection{Simulated Annealing}

The simulated annealing algorithm provides lots of configuration parameters
and it is difficult to input the best values of start temperature, delta
temperature and number of runs to archive the best error rates.

We tried various temperatures values and came up with the Table \ref{tbl:results_bayesian_networks_sa}.

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c |}
    \hline
    \textbf{Parameters} & \textbf{Train error rate} & \textbf{Test error rate} & \textbf{Time} \\ \hline
    SimulatedAnnealing -U 75 -D 0.4 -A 20 & 14.32 & 26.80 & 0m29.841s \\ \hline
    SimulatedAnnealing -U 55 -D 0.4 -A 20 & 14.40 & 21.45 & 0m29.77s \\ \hline
    SimulatedAnnealing -U 52 -D 0.4 -A 20 & 14.49 & 21.09 & 0m29.407s \\ \hline
    SimulatedAnnealing -U 50 -D 0.4 -A 20 & 15.2 & 21.35 & 0m30.884s \\ \hline
    SimulatedAnnealing -U 50 -D 0.4 -A 40 & 15.2 & 21.35 & 0m30.884s \\ \hline
    SimulatedAnnealing -U 25 -D 0.4 -A 20 & 18.25 & 22.31 & 0m29.602s \\ \hline
    SimulatedAnnealing -U 10 -D 0.4 -A 20 & 19.82 & 22.96 & 0m29.131s \\ \hline
  \end{tabular}
  \caption{Results for Bayesian Networks / Simulated Annealing.}
  \label{tbl:results_bayesian_networks_sa}
  \end{center}
\end{table}

The diminish the error rates more research will be required to find the best
tuple of configuration values. Anyway, with just a few guesses we came up
with $\approx21$\% error rate, which is quite good when comparing to other algorithms.

\section{Related Work}

\section{Conclusions}

\begin{thebibliography}{1}
\bibitem{1}
Mitchel, T.; Machine Learning; McGraw-Hill, 1997
\bibitem{2}
Han, J., Kamber, M., Pei, J.; Data Mining: Concepts and Techniques, Second Edition; Morgan Kaufmann, 2005
\bibitem{3}
H. Witten, I., Frank, E.; Data Mining: Practical Machine Learning Tools and Techniques, Second Edition; Morgan Kaufmann, 2005
\bibitem{4}
RapidMiner; \url{http://www.rapidminer.com/}, November 2009
\bibitem{c45}
Ross Quinlan, J.; C4.5: Programs for Machine Learning; Morgan Kaufmann, 1992
\bibitem{6}
The R Project for Statistical Computing; http://www.r-project.org/, November 2009
\bibitem{7}
Hahsler, M., Gruen, B.; Hornik, K.; arules -- A Computational Environment for Mining Association Rules and Frequent Item Sets; Journal of Statistical Software 14/15, 2005

\bibitem{id3}
Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.

\bibitem{cart}
Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (1984). Classification and Regression Trees. Wadsworth International Group, Belmont, California.

\bibitem{random_forest}
Liaw, Andy and Wiener, Matthew. Classification and Regression by randomForest. R News (2002) Vol. 2/3 p. 18

\bibitem{adtree}
Freund, Y., Mason, L.: The alternating decision tree learning algorithm. In: Proceeding of the Sixteenth International Conference on Machine Learning, Bled, Slovenia, 124-133, 1999

\bibitem{bayes}
N. Friedman, D. Geiger, M. Goldszmidt. Bayesian Network Classifiers. Machine Learning, 29: 131–163, 1997

\bibitem{rule_induction}
Quinlan, J. R. (1987). "Generating production rules from decision trees". in McDermott, John. Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87). Milan, Italy. pp. 304–307

\bibitem{aleph}
Ashwin Srinivasan. The Aleph Manual. 2007

\bibitem{bayes3}
W.L. Buntine. A guide to the literature on learning probabilistic networks from data. IEEE 
Transactions on Knowledge and Data Engineering, 8:195–210, 1996.

\bibitem{bayes_weka}
Remco R. Bouckaert. Bayesian Network Classifiers in Weka. 2004

\bibitem{weka}
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten (2009); The WEKA Data Mining Software: An Update; SIGKDD Explorations, Volume 11, Issue 1.

\bibitem{cn2}
Peter Clark and Tim Niblett. The CN2 induction algorithm. 1988.

\bibitem{cn2_laplace}
Peter Clark and Robin Boswell. Rule induction with CN2: Some recent improvements. In Y. Kodratoff, editor, Machine Learning - EWSL-91, pages 151-163, Berlin, 1991. Springer-Verlag.

\bibitem{ilp}
Nada Lavrac and Saso Dzeroski. Inductive Logic Programming: Techniques and Applications. Ellis Horwood, New York, 1994.

\bibitem{boosting}
Robert E. Schapire, The Boosting Approach to Machine Learning: An Overview, 2001.

\end{thebibliography}

\clearpage

\appendix

\section{Tables}

\clearpage
\section{Decision trees}

\begin{lstlisting}[language=c,frame=single,breaklines=true,basicstyle=\footnotesize\ttfamily,caption={Decision tree built by C4.5 with -C 0.05},label=decision_tree_c45]
capital_gain = none
|   marital_status = never_married: <50K (10228.0/340.0)
|   marital_status = married_civ_spouse
|   |   education = bachelors
|   |   |   capital_loss = none
|   |   |   |   age = young: <50K (33.0/4.0)
|   |   |   |   age = middle_aged: >50K (1287.0/515.0)
|   |   |   |   age = senior: >50K (707.0/243.0)
|   |   |   |   age = old: <50K (77.0/25.0)
|   |   |   capital_loss = low: >50K (0.0)
|   |   |   capital_loss = medium: >50K (222.0/26.0)
|   |   education = hs_grad
|   |   |   capital_loss = none: <50K (4167.0/1130.0)
|   |   |   capital_loss = low: <50K (1.0)
|   |   |   capital_loss = medium: >50K (235.0/105.0)
|   |   education = 11th: <50K (328.0/29.0)
|   |   education = masters: >50K (823.0/214.0)
|   |   education = 9th: <50K (212.0/18.0)
|   |   education = some_college: <50K (2508.0/1000.0)
|   |   education = assoc_acdm: <50K (412.0/186.0)
|   |   education = assoc_voc: <50K (596.0/251.0)
|   |   education = 7th_8th: <50K (325.0/29.0)
|   |   education = doctorate: >50K (230.0/42.0)
|   |   education = prof_school: >50K (295.0/62.0)
|   |   education = 5th_6th: <50K (161.0/11.0)
|   |   education = 10th
|   |   |   capital_loss = none: <50K (314.0/41.0)
|   |   |   capital_loss = low: <50K (0.0)
|   |   |   capital_loss = medium: >50K (9.0/2.0)
|   |   education = 1st_4th: <50K (78.0/5.0)
|   |   education = preschool: <50K (18.0)
|   |   education = 12th: <50K (119.0/21.0)
|   marital_status = divorced: <50K (4155.0/333.0)
|   marital_status = married_spouse_absent: <50K (397.0/25.0)
|   marital_status = separated: <50K (973.0/46.0)
|   marital_status = married_af_spouse
|   |   hours_per_week = part_time: <50K (2.0)
|   |   hours_per_week = full_time: <50K (3.0)
|   |   hours_per_week = over_time: >50K (13.0/4.0)
|   |   hours_per_week = workaholic: <50K (3.0)
|   marital_status = widowed: <50K (918.0/62.0)
capital_gain = low: <50K (55.0)
capital_gain = medium: <50K (1009.0/181.0)
capital_gain = high: >50K (878.0/138.0)
capital_gain = very_high: >50K (770.0/14.0)
\end{lstlisting}

\clearpage
\section{Classification Rules}
\begin{lstlisting}[language=c,frame=single,breaklines=true,basicstyle=\footnotesize\ttfamily,caption={Subset of the ordered set of rules induced by CN2 with the laplacian error estimation method},label=cn2_rules]
IF    age < 20.50
  AND capital_gain < 9720.00
  AND hours_per_week < 59.00
THEN  class = less_or_equal_than_50K  [0 2366]
ELSE
IF    19.50 < age < 21.50
  AND marital_status = never_married
  AND capital_gain < 67047.00
THEN  class = less_or_equal_than_50K  [0 664]
ELSE
IF    age < 54.50
  AND capital_gain > 10585.50
  AND hours_per_week > 33.00
THEN  class = more_than_50K  [520 0]
ELSE
IF    age < 23.50
  AND hours_per_week < 31.00
THEN  class = less_or_equal_than_50K  [0 492]
ELSE
IF    age < 24.50
  AND workclass = private
  AND relationship = own_child
  AND capital_loss < 2129.50
  AND hours_per_week > 32.50
  AND native_country = united_states
THEN  class = less_or_equal_than_50K  [0 562]
ELSE
IF    age < 60.50
  AND 7565.50 < capital_gain < 30961.50
THEN  class = more_than_50K  [445 0]
ELSE
IF    relationship = own_child
  AND race = black
  AND hours_per_week < 49.00
THEN  class = less_or_equal_than_50K  [0 261]
ELSE
IF    age < 38.50
  AND education = hs_grad
  AND marital_status = never_married
  AND relationship = own_child
  AND hours_per_week < 75.00
THEN  class = less_or_equal_than_50K  [0 378]
ELSE
IF    20.50 < age < 23.50
  AND workclass = private
  AND marital_status = never_married
  AND capital_loss < 2116.00
  AND hours_per_week > 32.50
THEN  class = less_or_equal_than_50K  [0 434]
\end{lstlisting}
\clearpage

\begin{lstlisting}[language=c,frame=single,breaklines=true,basicstyle=\footnotesize\ttfamily,caption={Subset of the prolog clauses produced by Aleph},label=ilp_clauses]
more_than_50K(A) :-
   middle_aged(A), very_high_capital_gain(A), is_native_of_united_states(A).
more_than_50K(A) :-
   education_prof_school(A), wife(A), workaholic(A).
more_than_50K(A) :-
   senior(A), work_private(A), very_high_capital_gain(A).
more_than_50K(A) :-
   senior(A), work_federal_gov(A), education_doctorate(A).
more_than_50K(A) :-
   education_doctorate(A), medium_capital_loss(A), workaholic(A).
more_than_50K(A) :-
   work_self_emp_inc(A), high_capital_gain(A), full_time_worker(A).
more_than_50K(A) :-
   middle_aged(A), education_masters(A), is_native_of_iran(A).
more_than_50K(A) :-
   senior(A), very_high_capital_gain(A), over_time_worker(A).
more_than_50K(A) :-
   other_relative(A), high_capital_gain(A).
more_than_50K(A) :-
   education_bachelors(A), occupation_other_service(A), high_capital_gain(A).
more_than_50K(A) :-
   senior(A), married_civ_spouse(A), very_high_capital_gain(A).
more_than_50K(A) :-
   senior(A), full_time_worker(A), is_native_of_cambodia(A).
more_than_50K(A) :-
   middle_aged(A), married_civ_spouse(A), is_native_of_thailand(A).
more_than_50K(A) :-
   young(A), high_capital_gain(A), over_time_worker(A).
more_than_50K(A) :-
   middle_aged(A), asian_pac_islander(A), high_capital_gain(A).
more_than_50K(A) :-
   occupation_protective_serv(A), high_capital_gain(A), full_time_worker(A).
more_than_50K(A) :-
   work_state_gov(A), occupation_exec_managerial(A), high_capital_gain(A).
more_than_50K(A) :-
   education_bachelors(A), occupation_tech_support(A), wife(A).
more_than_50K(A) :-
   education_prof_school(A), very_high_capital_gain(A).
more_than_50K(A) :-
   high_capital_gain(A), full_time_worker(A), is_native_of_canada(A).
more_than_50K(A) :-
   education_prof_school(A), medium_capital_loss(A), over_time_worker(A).
more_than_50K(A) :-
   work_private(A), education_doctorate(A), is_native_of_taiwan(A).
\end{lstlisting}


\end{document}
